{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Transformer Models — Inference & Evaluation\n",
    "\n",
    "This notebook evaluates the fine-tuned transformer models:\n",
    "\n",
    "- **CodeBERT**\n",
    "- **GraphCodeBERT**\n",
    "- **CodeT5**\n",
    "\n",
    "using the prepared dataset: `data/processed/merged_for_evaluation.csv`\n",
    "\n",
    "We compute:\n",
    "\n",
    "- Precision\n",
    "- Recall\n",
    "- F1‑Score\n",
    "- Hamming Loss\n",
    "- Subset Accuracy\n",
    "- PR‑Curves for every class\n",
    "\n",
    "Predictions are also exported to: `data/predictions/`\n",
    "\n",
    "This experiment allows us to compare transformer-based models with the classical baselines (Random Forest and XGBoost)."
   ],
   "id": "9f942634c7612718"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load dataset",
   "id": "90726a3a125b42d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T11:29:08.529241Z",
     "start_time": "2025-11-16T11:29:07.954929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.inference.load_evaluation_data import load_and_prepare_evaluation_data\n",
    "\n",
    "texts, y_true, label_cols = load_and_prepare_evaluation_data(\"../data/processed/merged_for_evaluation.csv\")"
   ],
   "id": "46014d1b49599bf9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load inference utilities",
   "id": "9151692154f96107"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T11:29:17.106954Z",
     "start_time": "2025-11-16T11:29:15.221851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.inference.transformer_models_inference import (\n",
    "    load_sequence_classification_model,\n",
    "    load_seq2seq_model,\n",
    "    predict_multilabel_classification,\n",
    "    predict_codet5,\n",
    "    evaluate_predictions,\n",
    "    plot_pr_curves,\n",
    "    save_predictions_csv\n",
    ")"
   ],
   "id": "df0f2a0bace73fe2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CodeBERT — Inference & Evaluation",
   "id": "e45dbc3a7fb69813"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T10:46:51.017752Z",
     "start_time": "2025-11-16T10:24:03.080506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"../models/transformers/codebert/codebert-base_multilabel_finetuned\"\n",
    "\n",
    "tokenizer, model = load_sequence_classification_model(model_path)\n",
    "\n",
    "y_pred, probs = predict_multilabel_classification(texts, tokenizer, model)\n",
    "\n",
    "report, ham_loss, subset_acc = evaluate_predictions(y_true, y_pred, label_cols)\n",
    "\n",
    "print(report)\n",
    "print(\"Hamming Loss:\", ham_loss)\n",
    "print(\"Subset Accuracy:\", subset_acc)\n",
    "\n",
    "plot_pr_curves(y_true, probs, label_cols, \"codebert\", \"../data/images\")\n",
    "save_predictions_csv(texts, y_true, y_pred, probs, label_cols, \"../data/predictions/codebert_predictions.csv\")"
   ],
   "id": "4245ce129832ac25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting load_sequence_classification_model...\n",
      "[INFO] Finished load_sequence_classification_model.\n",
      "[INFO] Starting predict_multilabel_classification...\n",
      "[INFO] Finished predict_multilabel_classification. Time taken: 1365.92 seconds.\n",
      "[INFO] Starting evaluate_predictions...\n",
      "[INFO] Finished evaluate_predictions. Time taken: 0.01 seconds.\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Long Method       0.34      0.36      0.35       314\n",
      "God/Large Class       0.60      0.05      0.09       866\n",
      "   Feature Envy       0.39      0.32      0.35       400\n",
      "     Data Class       0.62      0.61      0.61       656\n",
      "          Clean       0.94      0.98      0.96     19527\n",
      "\n",
      "      micro avg       0.91      0.91      0.91     21763\n",
      "      macro avg       0.58      0.46      0.47     21763\n",
      "   weighted avg       0.90      0.91      0.89     21763\n",
      "    samples avg       0.91      0.91      0.91     21763\n",
      "\n",
      "Hamming Loss: 0.036846264701780485\n",
      "Subset Accuracy: 0.910510901399284\n",
      "[INFO] Starting plot_pr_curves...\n",
      "[INFO] Finished plot_pr_curves. Time taken: 0.23 seconds.\n",
      "[INFO] Starting save_predictions_csv...\n",
      "✔️ Saved predictions to: ../data/predictions/codebert_predictions.csv\n",
      "[INFO] Finished save_predictions_csv. Time taken: 1.12 seconds.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GraphCodeBERT — Inference & Evaluation",
   "id": "afa5890c47c612ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T11:22:55.874278Z",
     "start_time": "2025-11-16T11:01:21.085769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"../models/transformers/graphcodebert/graphcodebert-base_multilabel_finetuned\"\n",
    "\n",
    "tokenizer, model = load_sequence_classification_model(model_path)\n",
    "\n",
    "y_pred, probs = predict_multilabel_classification(texts, tokenizer, model)\n",
    "\n",
    "report, ham_loss, subset_acc = evaluate_predictions(y_true, y_pred, label_cols)\n",
    "\n",
    "print(report)\n",
    "print(\"Hamming Loss:\", ham_loss)\n",
    "print(\"Subset Accuracy:\", subset_acc)\n",
    "\n",
    "plot_pr_curves(y_true, probs, label_cols, \"graphcodebert\", \"../data/images\")\n",
    "save_predictions_csv(texts, y_true, y_pred, probs, label_cols, \"../data/predictions/graphcodebert_predictions.csv\")"
   ],
   "id": "842cb859e0d3d912",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting load_sequence_classification_model...\n",
      "[INFO] Finished load_sequence_classification_model.\n",
      "[INFO] Starting predict_multilabel_classification...\n",
      "[INFO] Finished predict_multilabel_classification. Time taken: 1293.21 seconds.\n",
      "[INFO] Starting evaluate_predictions...\n",
      "[INFO] Finished evaluate_predictions. Time taken: 0.01 seconds.\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Long Method       0.00      0.00      0.00       314\n",
      "God/Large Class       0.00      0.00      0.00       866\n",
      "   Feature Envy       0.00      0.00      0.00       400\n",
      "     Data Class       0.00      0.00      0.00       656\n",
      "          Clean       0.92      0.99      0.95     19527\n",
      "\n",
      "      micro avg       0.92      0.89      0.90     21763\n",
      "      macro avg       0.18      0.20      0.19     21763\n",
      "   weighted avg       0.82      0.89      0.85     21763\n",
      "    samples avg       0.90      0.90      0.90     21763\n",
      "\n",
      "Hamming Loss: 0.03864999302682349\n",
      "Subset Accuracy: 0.901538747617498\n",
      "[INFO] Starting plot_pr_curves...\n",
      "[INFO] Finished plot_pr_curves. Time taken: 0.16 seconds.\n",
      "[INFO] Starting save_predictions_csv...\n",
      "✔️ Saved predictions to: ../data/predictions/graphcodebert_predictions.csv\n",
      "[INFO] Finished save_predictions_csv. Time taken: 1.10 seconds.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CodeT5 — Inference & Evaluation",
   "id": "b56909694b56b8b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T12:11:27.494643Z",
     "start_time": "2025-11-16T11:29:23.900927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "model_path = \"../models/transformers/codet5/codet5-base_multilabel_finetuned\"\n",
    "\n",
    "tokenizer, model = load_seq2seq_model(model_path)\n",
    "\n",
    "decoded, text_to_vec = predict_codet5(texts, tokenizer, model)\n",
    "\n",
    "# Convert decoded text labels → multilabel vectors\n",
    "y_pred = np.array([text_to_vec(s, label_cols) for s in decoded])\n",
    "\n",
    "# For PR curves, we cannot use text generation probabilities → set dummy equal to predictions\n",
    "probs = y_pred.astype(float)\n",
    "\n",
    "report, ham_loss, subset_acc = evaluate_predictions(y_true, y_pred, label_cols)\n",
    "\n",
    "print(report)\n",
    "print(\"Hamming Loss:\", ham_loss)\n",
    "print(\"Subset Accuracy:\", subset_acc)\n",
    "\n",
    "plot_pr_curves(y_true, probs, label_cols, \"codet5\", \"../data/images\")\n",
    "save_predictions_csv(texts, y_true, y_pred, probs, label_cols, \"../data/predictions/codet5_predictions.csv\")"
   ],
   "id": "7a74a1acac21eeba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting load_seq2seq_model...\n",
      "[INFO] Finished load_seq2seq_model.\n",
      "[INFO] Starting predict_codet5...\n",
      "[INFO] Finished predict_codet5. Time taken: 2521.94 seconds.\n",
      "[INFO] Starting evaluate_predictions...\n",
      "[INFO] Finished evaluate_predictions. Time taken: 0.01 seconds.\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Long Method       0.65      0.11      0.20       314\n",
      "God/Large Class       0.61      0.22      0.32       866\n",
      "   Feature Envy       0.71      0.11      0.19       400\n",
      "     Data Class       0.79      0.70      0.75       656\n",
      "          Clean       0.94      0.99      0.96     19527\n",
      "\n",
      "      micro avg       0.93      0.92      0.93     21763\n",
      "      macro avg       0.74      0.43      0.48     21763\n",
      "   weighted avg       0.91      0.92      0.91     21763\n",
      "    samples avg       0.93      0.93      0.93     21763\n",
      "\n",
      "Hamming Loss: 0.030142717679326855\n",
      "Subset Accuracy: 0.9292919901445772\n",
      "[INFO] Starting plot_pr_curves...\n",
      "[INFO] Finished plot_pr_curves. Time taken: 0.16 seconds.\n",
      "[INFO] Starting save_predictions_csv...\n",
      "✔️ Saved predictions to: ../data/predictions/codet5_predictions.csv\n",
      "[INFO] Finished save_predictions_csv. Time taken: 1.11 seconds.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Transformer Models Performance Analysis (Code Smell Detection)\n",
    "\n",
    "We evaluated three transformer-based models — **CodeBERT**, **GraphCodeBERT**, and **CodeT5** — on the multilabel classification task of code smell detection using the test set derived from the `SmellyCode++` dataset.\n",
    "\n",
    "#### CodeBERT\n",
    "- **Overall performance** was robust, particularly for the `Clean` class (`F1 = 0.96`).\n",
    "- CodeBERT demonstrated moderate ability to detect `Data Class` (F1 = 0.61) and `Feature Envy` (F1 = 0.35), with relatively low effectiveness for `Long Method` (F1 = 0.35) and `God/Large Class` (F1 = 0.09).\n",
    "- **Precision-Recall (PR) curves** confirmed the strong separability of `Clean` and `Data Class`, while other smell classes exhibited limited precision across the recall spectrum.\n",
    "\n",
    "#### GraphCodeBERT\n",
    "- While achieving strong results on `Clean` code (`F1 = 0.95`), this model failed to identify any of the code smell classes (`F1 = 0.00` for all).\n",
    "- The PR curves show near-zero performance for smell labels — suggesting that the model overfitted to the majority class.\n",
    "- **Subset accuracy** (0.90) and **Hamming Loss** (≈0.038) remained decent, solely due to dominance of `Clean` samples in the dataset.\n",
    "\n",
    "#### CodeT5\n",
    "- **Best overall multilabel performance** across all classes among the three models.\n",
    "- Achieved the **highest scores on code smell classes**, particularly `Data Class` (F1 = 0.75), and `God/Large Class` (F1 = 0.32).\n",
    "- Performed well for `Clean` (F1 = 0.96), maintaining high recall and precision.\n",
    "- PR curves demonstrated relatively **consistent performance across all labels**, unlike the other models.\n",
    "- **Subset Accuracy: 92.9%**, **Lowest Hamming Loss** (0.030), and **macro F1 = 0.48**, confirming better balance across classes.\n",
    "\n",
    "#### Summary\n",
    "- **CodeT5** outperformed both CodeBERT and GraphCodeBERT on most evaluation metrics, especially on minority code smell classes.\n",
    "- **GraphCodeBERT** underperformed due to high class imbalance sensitivity.\n",
    "- **CodeBERT** remains a strong baseline, but its performance drops significantly on complex smell categories like `God Class` and `Long Method`.\n",
    "\n",
    "These findings suggest that sequence-to-sequence models (like CodeT5) may better capture inter-token patterns required to detect subtle design flaws in source code. Further comparison with classical models (e.g., Random Forest, XGBoost) will help confirm this hypothesis."
   ],
   "id": "28a02eaead847434"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
