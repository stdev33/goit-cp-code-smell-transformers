{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Inference Pipeline for Classical Models\n",
    "\n",
    "This notebook performs inference of trained classical machine learning models (Random Forest, XGBoost) on the evaluation dataset `merged_for_evaluation.csv`. The pipeline supports:\n",
    "- Model selection (Random Forest or XGBoost)\n",
    "- Metric calculation (Precision, Recall, F1-score, Hamming Loss, Subset Accuracy)\n",
    "- Optional saving of predictions to a CSV file\n",
    "\n",
    "This step evaluates the **generalization performance** of classical models on unseen code metrics from *SmellyCode++*, separated earlier for evaluation purposes."
   ],
   "id": "32a9bf3c8cdbf835"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Evaluation Dataset\n",
    "\n",
    "We load the pre-split evaluation dataset containing only samples from *SmellyCode++*, previously excluded from model training. This dataset includes code metrics and binary labels for multiple code smells."
   ],
   "id": "50d7ee154ea3f1cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:28:48.920294Z",
     "start_time": "2025-11-09T08:28:48.337593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_eval = pd.read_csv(\"../data/processed/merged_for_evaluation.csv\")\n",
    "print(f\"Evaluation dataset shape: {df_eval.shape}\")"
   ],
   "id": "9d38aef991399014",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset shape: (21511, 18)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run Inference with Classical Model\n",
    "\n",
    "Choose a trained model (`random_forest` or `xgboost`) and run the inference pipeline to:\n",
    "- Predict labels for each code smell\n",
    "- Evaluate model performance with standard metrics\n",
    "- Optionally export predictions to a CSV file"
   ],
   "id": "3da0850966c15c0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:29:01.553694Z",
     "start_time": "2025-11-09T08:28:51.159963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.inference.classical_models_predict_smells import run_inference_pipeline\n",
    "\n",
    "\n",
    "# model_type: \"random_forest\" or \"xgboost\"\n",
    "# save_path: file path to save predictions CSV, or None to skip saving\n",
    "run_inference_pipeline(\n",
    "    df=df_eval,\n",
    "    model_type=\"random_forest\",\n",
    "    save_path=\"../data/predictions/random_forest_predictions.csv\"\n",
    ")"
   ],
   "id": "774bfcb77c2d6134",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RANDOM_FOREST Inference Evaluation ===\n",
      "\n",
      "--- Label: Long Method ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     21197\n",
      "           1       0.04      0.04      0.04       314\n",
      "\n",
      "    accuracy                           0.97     21511\n",
      "   macro avg       0.51      0.51      0.51     21511\n",
      "weighted avg       0.97      0.97      0.97     21511\n",
      "\n",
      "\n",
      "--- Label: God Class ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     20645\n",
      "           1       0.69      0.48      0.56       866\n",
      "\n",
      "    accuracy                           0.97     21511\n",
      "   macro avg       0.84      0.73      0.77     21511\n",
      "weighted avg       0.97      0.97      0.97     21511\n",
      "\n",
      "\n",
      "--- Label: Feature Envy ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98     21111\n",
      "           1       0.17      0.23      0.20       400\n",
      "\n",
      "    accuracy                           0.97     21511\n",
      "   macro avg       0.58      0.61      0.59     21511\n",
      "weighted avg       0.97      0.97      0.97     21511\n",
      "\n",
      "\n",
      "--- Label: Data Class ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99     20855\n",
      "           1       0.70      0.13      0.22       656\n",
      "\n",
      "    accuracy                           0.97     21511\n",
      "   macro avg       0.84      0.57      0.60     21511\n",
      "weighted avg       0.97      0.97      0.96     21511\n",
      "\n",
      "\n",
      "--- Label: Clean ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.33      0.40      1984\n",
      "           1       0.93      0.97      0.95     19527\n",
      "\n",
      "    accuracy                           0.91     21511\n",
      "   macro avg       0.72      0.65      0.68     21511\n",
      "weighted avg       0.90      0.91      0.90     21511\n",
      "\n",
      "Hamming Loss: 0.0426\n",
      "Subset Accuracy (Exact Match): 0.8927\n",
      "✔️ Predictions saved to: ../data/predictions/random_forest_predictions.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T08:37:32.382187Z",
     "start_time": "2025-11-09T08:37:31.280123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.inference.classical_models_predict_smells import run_inference_pipeline\n",
    "\n",
    "\n",
    "# model_type: \"random_forest\" or \"xgboost\"\n",
    "# save_path: file path to save predictions CSV, or None to skip saving\n",
    "run_inference_pipeline(\n",
    "    df=df_eval,\n",
    "    model_type=\"xgboost\",\n",
    "    save_path=\"../data/predictions/xgboost_predictions.csv\"\n",
    ")"
   ],
   "id": "26c16351f10d8222",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBOOST Inference Evaluation ===\n",
      "\n",
      "--- Label: Long Method ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     21197\n",
      "           1       0.80      0.01      0.03       314\n",
      "\n",
      "    accuracy                           0.99     21511\n",
      "   macro avg       0.89      0.51      0.51     21511\n",
      "weighted avg       0.98      0.99      0.98     21511\n",
      "\n",
      "\n",
      "--- Label: God Class ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     20645\n",
      "           1       0.70      0.50      0.59       866\n",
      "\n",
      "    accuracy                           0.97     21511\n",
      "   macro avg       0.84      0.75      0.79     21511\n",
      "weighted avg       0.97      0.97      0.97     21511\n",
      "\n",
      "\n",
      "--- Label: Feature Envy ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     21111\n",
      "           1       1.00      0.01      0.01       400\n",
      "\n",
      "    accuracy                           0.98     21511\n",
      "   macro avg       0.99      0.50      0.50     21511\n",
      "weighted avg       0.98      0.98      0.97     21511\n",
      "\n",
      "\n",
      "--- Label: Data Class ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     20855\n",
      "           1       0.44      0.05      0.09       656\n",
      "\n",
      "    accuracy                           0.97     21511\n",
      "   macro avg       0.71      0.52      0.54     21511\n",
      "weighted avg       0.95      0.97      0.96     21511\n",
      "\n",
      "\n",
      "--- Label: Clean ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.33      0.31      1984\n",
      "           1       0.93      0.91      0.92     19527\n",
      "\n",
      "    accuracy                           0.86     21511\n",
      "   macro avg       0.61      0.62      0.62     21511\n",
      "weighted avg       0.87      0.86      0.87     21511\n",
      "\n",
      "Hamming Loss: 0.0463\n",
      "Subset Accuracy (Exact Match): 0.8437\n",
      "✔️ Predictions saved to: ../data/predictions/xgboost_predictions.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Inference Results Analysis\n",
    "\n",
    "This section presents an evaluation of **Random Forest** and **XGBoost** models on the unseen `merged_for_evaluation.csv` dataset. These results validate how well the models generalize beyond the training set.\n",
    "The goal is to assess the real-world performance of our classical models on new code samples, including both **smelly** and **clean** classes. These insights will serve as a benchmark for comparing against transformer-based models in the next phase of the study.\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest Results\n",
    "\n",
    "| Code Smell     | Precision | Recall | F1-score | Support |\n",
    "|----------------|-----------|--------|----------|---------|\n",
    "| Long Method    | 0.04      | 0.04   | 0.04     | 314     |\n",
    "| God Class      | 0.69      | 0.48   | 0.56     | 866     |\n",
    "| Feature Envy   | 0.17      | 0.23   | 0.20     | 400     |\n",
    "| Data Class     | 0.70      | 0.13   | 0.22     | 656     |\n",
    "| Clean          | 0.93      | 0.97   | 0.95     | 19,527  |\n",
    "| **Macro Avg**  | **0.51**  | **0.51**| **0.51** | -       |\n",
    "\n",
    "- **Hamming Loss**: 0.0426\n",
    "- **Subset Accuracy**: 89.27%\n",
    "\n",
    "Predictions saved to: `../data/predictions/random_forest_predictions.csv`\n",
    "\n",
    "---\n",
    "\n",
    "### XGBoost Results\n",
    "\n",
    "| Code Smell     | Precision | Recall | F1-score | Support |\n",
    "|----------------|-----------|--------|----------|---------|\n",
    "| Long Method    | 0.80      | 0.01   | 0.03     | 314     |\n",
    "| God Class      | 0.70      | 0.50   | 0.59     | 866     |\n",
    "| Feature Envy   | 1.00      | 0.01   | 0.01     | 400     |\n",
    "| Data Class     | 0.44      | 0.05   | 0.09     | 656     |\n",
    "| Clean          | 0.93      | 0.91   | 0.92     | 19,527  |\n",
    "| **Macro Avg**  | **0.61**  | **0.62**| **0.62** | -       |\n",
    "\n",
    "- **Hamming Loss**: 0.0463\n",
    "- **Subset Accuracy**: 84.37%\n",
    "\n",
    "Predictions saved to: `../data/predictions/random_forest_predictions.csv`\n",
    "\n",
    "---\n",
    "\n",
    "### Insights\n",
    "\n",
    "- **Clean samples** are detected reliably by both models due to their overwhelming presence in the dataset.\n",
    "- Both models **struggle to detect rare smells**, especially *Long Method* and *Feature Envy*. Their poor F1-scores are mainly caused by very low recall.\n",
    "- **Random Forest** shows slightly better balance between recall and precision for minority classes, while **XGBoost** often fails to recall positive samples.\n",
    "- The large gap in detection performance highlights the limitations of classical models, especially when relying solely on static code metrics.\n",
    "\n",
    "These findings further support the need for **context-aware models**, such as **transformer-based architectures**, which can capture semantic and structural nuances of source code beyond shallow syntactic features."
   ],
   "id": "dbeadd7c4d7df6c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
